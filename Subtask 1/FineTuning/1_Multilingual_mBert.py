# -*- coding: utf-8 -*-
"""Copy of Semeeval_3_1_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2_dE3-T7uhGC0FYlhXYbRVIPOIdqhRL
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -qq '/content/drive/MyDrive/data.zip'

!pip install -q -U --no-cache-dir gdown --pre
!pip install -q transformers datasets evaluate accelerate
!pip install -q requests nlpaug sentencepiece

# Standard modules

import os
import numpy as np
import pandas as pd

from tqdm import tqdm

# Pytorch

from torch import nn, tensor

# Hugging face

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset, ClassLabel, load_metric
import evaluate

"""## Import data

"""

from sklearn.metrics import f1_score, precision_score, recall_score
from datasets import DatasetDict
from tqdm import tqdm
import time

def make_dataframe(input_folder, labels_folder=None):
    #MAKE TXT DATAFRAME
    text = []

    for fil in tqdm(filter(lambda x: x.endswith('.txt'), os.listdir(input_folder))):

        iD, txt = fil[7:].split('.')[0], open(input_folder +fil, 'r', encoding='utf-8').read()
        text.append((iD, txt))

    df_text = pd.DataFrame(text, columns=['id','text']).set_index('id')

    df = df_text

    #MAKE LABEL DATAFRAME
    if labels_folder:
        labels = pd.read_csv(labels_folder, sep='\t', header=None)
        labels = labels.rename(columns={0:'id',1:'type'})
        labels.id = labels.id.apply(str)
        labels = labels.set_index('id')

        #JOIN
        df = labels.join(df_text)[['text','type']]

    return df
def read_lang_data(train_folder, train_labels, val_folder, val_labels):
  # read train data
  df_train_lang = make_dataframe(train_folder, train_labels)
  df_train_lang = df_train_lang.rename(columns={"type" : "label"})

  # read test data
  df_val_lang = make_dataframe(val_folder, val_labels)
  df_val_lang = df_val_lang.rename(columns={"type" : "label"})

  return df_train_lang, df_val_lang


recall_metric = evaluate.load("recall")
precision_metric = evaluate.load("precision")
accuracy_metric = evaluate.load("accuracy")

def eval_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)

    results = {}
    results.update(accuracy_metric.compute(predictions=preds, references = labels))
    results.update(recall_metric.compute(predictions=preds, references=labels, average="macro"))
    results.update(precision_metric.compute(predictions=preds, references=labels, average="macro"))

    # Calculate macro F1 score
    f1_macro = f1_score(labels, preds, average="macro")
    results.update({"eval_f1_macro": f1_macro})

    # Calculate micro F1 score
    f1_micro = f1_score(labels, preds, average="micro")
    results.update({"eval_f1_micro": f1_micro})

    return results



data_dict = {}
languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']

for lang in languages:

  train_folder = f"data/{lang}/train-articles-subtask-1/"
  train_labels = f"data/{lang}/train-labels-subtask-1.txt"
  dev_folder =  f"data/{lang}/dev-articles-subtask-1/"
  dev_labels =  f"data/{lang}/dev-labels-subtask-1.txt"

  df_train, df_dev = read_lang_data(train_folder, train_labels, dev_folder, dev_labels)

  data_dict[lang] = {'train': df_train, 'dev': df_dev, 'combined': pd.concat([df_train, df_dev])}

datasets = {}

for key,el in data_dict.items():
  labels = ['opinion', 'satire', 'reporting']
  ClassLabels = ClassLabel(num_classes=len(labels), names=labels)

  # Create hugging face dataset, adjusted to torch format and splitted for train/val in 80/20 ratio
  dataset = Dataset.from_pandas(el['combined'], preserve_index=True).cast_column("label", ClassLabels).train_test_split(test_size=0.2)
  val_dataset = Dataset.from_pandas(el['dev'], preserve_index=True).cast_column("label", ClassLabels)
  # combined_dataset = Dataset.from_pandas(el['combined'], preserve_index=True).cast_column("label", ClassLabels)

  datasets[key] = DatasetDict({'train': dataset['train'], 'test': dataset['test'], 'val': val_dataset})

datasets['en']['train']

datasets['en']['train']

from sklearn.model_selection import train_test_split

tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

from sklearn.utils.class_weight import compute_class_weight
import torch
# datasets
tokenized_datasets = {}

for key,el in datasets.items():
    tokenized_datasets[key] = el.map(tokenize_function, batched=True, remove_columns=["text"])

# tokenized_datasets
model = AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-uncased", num_labels=len(labels))

def model_init_bert():
    #return AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-uncased", hidden_dropout_prob=0.4, attention_probs_dropout_prob=0.4, num_labels=len(labels))
    return AutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-uncased", num_labels=len(labels))

merged_train_set = pd.DataFrame()
merged_test_set = pd.DataFrame()
merged_val_set = pd.DataFrame()

df_train_list = []
df_test_list = []
df_val_list = []

for key,el in tokenized_datasets.items():

    df_train_list.append(el['train'].to_pandas())
    df_test_list.append(el['test'].to_pandas())
    df_val_list.append(el['val'].to_pandas())

merged_train_set = pd.concat(df_train_list)
merged_test_set = pd.concat(df_test_list)
merged_val_set = pd.concat(df_val_list)

train_dataset = Dataset.from_pandas(merged_train_set)
test_dataset = Dataset.from_pandas(merged_test_set)
val_dataset = Dataset.from_pandas(merged_val_set)

y = train_dataset['label']

class_weights=compute_class_weight(class_weight='balanced', classes=np.unique(y), y=np.asarray(y))
class_weights=tensor(class_weights,dtype=torch.float).cuda()
print(class_weights)

from torch.utils.data import DataLoader

def create_dataloader(dataset, batch_size, shuffle=False, num_workers=0, pin_memory=False):
    """
    Create a DataLoader with specified parameters.

    Args:
        dataset (Dataset): The dataset from which to load the data.
        batch_size (int): How many samples per batch to load.
        shuffle (bool): Set to True to have the data reshuffled at every epoch.
        num_workers (int): How many subprocesses to use for data loading.
        pin_memory (bool): If True, the data loader will copy Tensors into CUDA pinned memory before returning them.

    Returns:
        DataLoader: The DataLoader with the specified configuration.
    """
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)

from sklearn.model_selection import ParameterGrid
import matplotlib.pyplot as plt
from transformers import TrainingArguments, Trainer
import multiprocessing

languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']

num_workers  = multiprocessing.cpu_count()

# Define the hyperparameter grid for tuning
hyperparameter_grid = {
    "num_train_epochs": [10, 15],
    "learning_rate": [1e-5, 2e-5, 3e-5, 4e-5],
    "per_device_batch_size": [8,16],
    #"weight_decay":[0.1, 0.2, 0.3],
}

best_params_global = None
best_f1_macro_global = 0.0
all_results = []

best_evaluate_results = {}

# Lists to store results for plotting
iterations = []
avg_f1_macros = []

# Perform grid search
for i, params in enumerate(ParameterGrid(hyperparameter_grid)):
    print('-----' * 4)
    print(f"Iteration {i + 1}: Training with hyperparameters {params}")

    # Training and evaluating BERT
    training_args = TrainingArguments(
        output_dir="output_trainer",
        evaluation_strategy="epoch",
        logging_strategy="epoch",
        save_strategy = "epoch",
        skip_memory_metrics=True,
        save_total_limit=1,  # Keep only the best model
        load_best_model_at_end=True,
        num_train_epochs=params["num_train_epochs"],
        per_device_train_batch_size=params["per_device_batch_size"],
        per_device_eval_batch_size=params["per_device_batch_size"],
        learning_rate=params["learning_rate"],
#        warmup_steps=params["warmup_steps"],
#        weight_decay=params["weight_decay"],
        report_to="all",
#        optim="adamw_torch"
    )

    train_loader = create_dataloader(train_dataset, batch_size=params["per_device_batch_size"], shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = create_dataloader(test_dataset, batch_size=params["per_device_batch_size"], num_workers=num_workers, pin_memory=True)

    trainer = Trainer(
        model_init=model_init_bert,
        args=training_args,
        train_dataset=train_loader.dataset,
        eval_dataset=val_loader.dataset,
        compute_metrics=eval_metrics,
    )

    trainer.train()

    # Evaluate on the combined validation set for parameter tuning
    evaluate_results = {}
    for lang in languages:
        ans_combined = trainer.evaluate(tokenized_datasets[lang]['val'])
        evaluate_results[lang] = ans_combined

    # Calculate average F1 macro across all languages
    avg_f1_macro = sum(result['eval_f1_macro'] for result in evaluate_results.values()) / len(languages)

    all_results.append({
        'hyperparameters': params,
        'avg_f1_macro': avg_f1_macro,
        'individual_results': evaluate_results,
        'trainers': trainer
    })

    # Update lists for plotting
    iterations.append(str(params))
    avg_f1_macros.append(avg_f1_macro)

    # Update best parameters if the current set performs better globally
    if avg_f1_macro > best_f1_macro_global:
        best_f1_macro_global = avg_f1_macro
        best_params_global = params
        best_evaluate_results = evaluate_results

print('\n' * 3)
print("Best global hyperparameters:", best_params_global)
print("Best global F1 macro:", best_f1_macro_global)

# Visualize results
plt.plot(iterations, avg_f1_macros, marker='o')
plt.xlabel('Iteration')
plt.ylabel('Average F1 Macro')
plt.title('Grid Search for Hyperparameter Tuning')
plt.xticks(rotation=90)  # Rotate x-axis tick labels
plt.show()

for run in all_results:
    hyperparams = run['hyperparameters']
    avg_f1_macro = run['avg_f1_macro']

    print(f"Hyperparameters: {hyperparams}")
    print(f"Average F1 Macro: {avg_f1_macro}")

    for lang, metrics in run['individual_results'].items():
        eval_f1_macro = metrics['eval_f1_macro']
        print(f"Language: {lang}, Eval F1 Macro: {eval_f1_macro}")

    print('---' * 10)  # Separator between runs

# best is second, so write 1
trainer = all_results[3]['trainers']

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have evaluate_results dictionary containing macro and micro F1 scores for each language
languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']

# Extract F1 scores for each language
f1_macro_scores = [best_evaluate_results[lang]['eval_f1_macro'] for lang in languages]
f1_micro_scores = [best_evaluate_results[lang]['eval_f1_micro'] for lang in languages]

# Set the width of the bars
bar_width = 0.35

# Set the positions for the bars on X-axis
r1 = np.arange(len(languages))
r2 = [x + bar_width for x in r1]

# Create grouped bar plot
bars1 = plt.bar(r1, f1_macro_scores, width=bar_width, alpha=0.8, label='F1 Macro')
bars2 = plt.bar(r2, f1_micro_scores, width=bar_width, alpha=0.8, label='F1 Micro')

# Add labels, title, and legend
plt.xlabel('Language', fontweight='bold')
plt.xticks([r + bar_width/2 for r in range(len(languages))], languages)
plt.ylabel('F1 Score')
plt.title('F1 Scores for Each Language (Macro and Micro)')
plt.legend()

for bar in bars1:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, round(yval, 3), ha='center', va='bottom')


for bar in bars2:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, round(yval, 3), ha='center', va='bottom')

# Show the plot
plt.show()

"""# Predict Test"""

data_pred_dict = {}
languages = [ 'en','es', 'fr', 'ge', 'gr', 'it', 'ka', 'po', 'ru']
label_mapping = {0: 'opinion', 1: 'satire', 2: 'reporting'}

# Replace the values in the 'label' column

for lang in languages:

  predict_folder = f"data/{lang}/test-articles-subtask-1/"

  df_pred_lang = make_dataframe(predict_folder, labels_folder = None)

  data_pred_dict[lang] = {'pred': df_pred_lang}

# All labels

pred_datasets = {}

for key,el in data_pred_dict.items():
  pred_dataset = Dataset.from_pandas(el['pred'], preserve_index=True)

  pred_datasets[key] = pred_dataset

pred_tokenized_datasets = {}

for key,el in pred_datasets.items():
    pred_tokenized_datasets[key] = el.map(tokenize_function, batched=True, remove_columns=["text"])

# pred_results = {}
languages = [ 'en','es', 'fr', 'ge', 'gr', 'it', 'ka', 'po', 'ru']

for language in languages:
    pred_ans = trainer.predict(pred_tokenized_datasets[language])

    max_indices = np.argmax(pred_ans[0], axis=1)
    indexes = data_pred_dict[language]['pred'].index.tolist()

    pred_ans_df = pd.DataFrame({'Index': indexes, 'Value': max_indices})

    pred_ans_df['Value'] = pred_ans_df['Value'].replace(label_mapping)


    # pred_results[language] = pred_ans_df

    pred_ans_df.to_csv(f'ST1_hyperparam_{language}.txt', sep='\t', index=False, header=False)