# -*- coding: utf-8 -*-
"""Monolang_xml_RoBERTa_base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LKrLod7idx92TcpEkqxmqxI8CaEJwuKb
"""

!pip install numpy pandas tqdm
!pip install torch
!pip install transformers datasets evaluate
!pip install scikit-learn
!pip install matplotlib

from google.colab import drive
drive.mount('/content/drive')

!unzip -qq '/content/drive/MyDrive/data.zip'

# Standard modules
import os
import numpy as np
import pandas as pd
from tqdm import tqdm

# PyTorch
import torch
from torch import nn, tensor

# Hugging Face
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset, ClassLabel, DatasetDict
import evaluate

# Scikit-learn
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split, ParameterGrid
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import StratifiedShuffleSplit

# Matplotlib
import matplotlib.pyplot as plt

"""## Import data

"""

# Function to create a DataFrame from text files
def make_dataframe(input_folder, labels_folder=None):
    text = []
    for fil in tqdm(filter(lambda x: x.endswith('.txt'), os.listdir(input_folder))):
        iD, txt = fil[7:].split('.')[0], open(input_folder + fil, 'r', encoding='utf-8').read()
        text.append((iD, txt))

    df_text = pd.DataFrame(text, columns=['id', 'text']).set_index('id')

    if labels_folder:
        labels = pd.read_csv(labels_folder, sep='\t', header=None)
        labels = labels.rename(columns={0: 'id', 1: 'type'})
        labels.id = labels.id.apply(str)
        labels = labels.set_index('id')
        df = labels.join(df_text)[['text', 'type']]
    else:
        df = df_text

    return df

# Function to read language data
def read_lang_data(train_folder, train_labels, val_folder, val_labels):
    df_train_lang = make_dataframe(train_folder, train_labels).rename(columns={"type": "label"})
    df_val_lang = make_dataframe(val_folder, val_labels).rename(columns={"type": "label"})
    return df_train_lang, df_val_lang

# Preparing the data dictionary
data_dict = {}
languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']

for lang in languages:

  train_folder = f"data/{lang}/train-articles-subtask-1/"
  train_labels = f"data/{lang}/train-labels-subtask-1.txt"
  val_folder =  f"data/{lang}/dev-articles-subtask-1/"
  val_labels =  f"data/{lang}/dev-labels-subtask-1.txt"

  df_train, df_test = read_lang_data(train_folder, train_labels, val_folder, val_labels)

  data_dict[lang] = {'train': df_train, 'val': df_test, 'combined': pd.concat([df_train, df_test])}

# Display the data
print(data_dict['en']['train'].head())

data_dict['en']['train'].head()

data_dict['en']['val'].head()

# Creating datasets for each language
datasets = {}
labels = ['opinion', 'satire', 'reporting']
ClassLabels = ClassLabel(num_classes=len(labels), names=labels)

for key, el in data_dict.items():
    df_train = el['train']
    df_val = el['val']

    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

    for train_index, test_index in stratified_split.split(df_train, df_train['label']):
        stratified_train = df_train.iloc[train_index]
        stratified_test = df_train.iloc[test_index]

    train_dataset = Dataset.from_pandas(stratified_train, preserve_index=True).cast_column("label", ClassLabels)
    test_dataset = Dataset.from_pandas(stratified_test, preserve_index=True).cast_column("label", ClassLabels)
    val_dataset = Dataset.from_pandas(df_val, preserve_index=True).cast_column("label", ClassLabels)

    datasets[key] = DatasetDict({'train': train_dataset, 'test': test_dataset, 'val': val_dataset})

# Loading evaluation metrics
recall_metric = evaluate.load("recall")
precision_metric = evaluate.load("precision")
accuracy_metric = evaluate.load("accuracy")

datasets['en']

def eval_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)

    results = {}
    results.update(accuracy_metric.compute(predictions=preds, references=labels))
    results.update(recall_metric.compute(predictions=preds, references=labels, average="macro"))
    results.update(precision_metric.compute(predictions=preds, references=labels, average="macro"))
    results.update({"eval_f1_macro": f1_score(labels, preds, average="macro")})
    results.update({"eval_f1_micro": f1_score(labels, preds, average="micro")})

    return results

"""## Xlm-Roberta-base model"""

def model_init_xlm():
    return AutoModelForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=len(labels))

tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenizing datasets
tokenized_datasets = {}
for key, el in datasets.items():
    tokenized_datasets[key] = el.map(tokenize_function, batched=True, remove_columns=["text"])

# Model initialization
model = AutoModelForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=len(labels))

tokenized_datasets

# Create merged train set
df_train_list = [el['train'].to_pandas() for el in tokenized_datasets.values()]
df_test_list = [el['test'].to_pandas() for el in tokenized_datasets.values()]
df_val_list = [el['val'].to_pandas() for el in tokenized_datasets.values()]

"""## Training

"""

models = []
evaluate_results_list = []

for i_lang, (train_set, test_set, val_set) in enumerate(zip(df_train_list, df_test_list, df_val_list)):
    train_dataset = Dataset.from_pandas(train_set)
    test_dataset = Dataset.from_pandas(test_set)
    val_dataset = Dataset.from_pandas(val_set)

    print(languages[i_lang])

    y = train_dataset['label']
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=np.asarray(y))
    class_weights = tensor(class_weights, dtype=torch.float).cuda()

    hyperparameter_grid = {
        "num_train_epochs": [10, 15],
        "learning_rate": [ 3e-5, 4e-5],
        "per_device_batch_size": [8, 16],
    }

    best_params_global = None
    best_f1_macro_global = 0.0
    all_results = []

    best_evaluate_results = {}

    iterations = []
    avg_f1_macros = []

    for i, params in enumerate(ParameterGrid(hyperparameter_grid)):
        print('-----' * 4)
        print(f"Iteration {i + 1}: Training with hyperparameters {params}")

        training_args = TrainingArguments(
            output_dir="output_trainer",
            evaluation_strategy="epoch",
            logging_strategy="epoch",
            skip_memory_metrics=True,
            num_train_epochs=params["num_train_epochs"],
            per_device_train_batch_size=params["per_device_batch_size"],
            per_device_eval_batch_size=params["per_device_batch_size"],
            learning_rate=params["learning_rate"],
            report_to="all"
        )

        trainer = Trainer(
            model_init=model_init_xlm,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            compute_metrics=eval_metrics,
        )

        trainer.train()
        models.append(trainer)

        evaluate_results = {}
        for lang in languages:
            ans_combined = trainer.evaluate(tokenized_datasets[lang]['val'])
            evaluate_results[lang] = ans_combined

        evaluate_results_list.append(evaluate_results)

        avg_f1_macro = sum(result['eval_f1_macro'] for result in evaluate_results.values()) / len(languages)

        all_results.append({
            'hyperparameters': params,
            'avg_f1_macro': avg_f1_macro,
            'individual_results': evaluate_results
        })

        iterations.append(str(params))
        avg_f1_macros.append(avg_f1_macro)

        if avg_f1_macro > best_f1_macro_global:
            best_f1_macro_global = avg_f1_macro
            best_params_global = params
            best_evaluate_results = evaluate_results

    print('\n' * 3)
    print("Best global hyperparameters:", best_params_global)
    print("Best global F1 macro:", best_f1_macro_global)

    plt.plot(iterations, avg_f1_macros, marker='o')
    plt.xlabel('Iteration')
    plt.ylabel('Average F1 Macro')
    plt.title('Grid Search for Hyperparameter Tuning')
    plt.xticks(rotation=90)
    plt.show()



"""##### Evaluate each dataset"""

evaluate_results_final = {}
for lang, res in zip(languages, evaluate_results_list):
    evaluate_results_final[lang] = res[lang]

f1_macro_scores = [evaluate_results_final[lang]['eval_f1_macro'] for lang in languages]
f1_micro_scores = [evaluate_results_final[lang]['eval_f1_micro'] for lang in languages]

bar_width = 0.35
r1 = np.arange(len(languages))
r2 = [x + bar_width for x in r1]

bars1 = plt.bar(r1, f1_macro_scores, width=bar_width, alpha=0.8, label='F1 Macro')
bars2 = plt.bar(r2, f1_micro_scores, width=bar_width, alpha=0.8, label='F1 Micro')

plt.xlabel('Language', fontweight='bold')
plt.xticks([r + bar_width / 2 for r in range(len(languages))], languages)
plt.ylabel('F1 Score')
plt.title('F1 Scores for Each Language (Macro and Micro)')
plt.legend()

"""# Predict Test"""

data_pred_dict = {}


#languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']
label_mapping = {0: 'opinion', 1: 'satire', 2: 'reporting'}

# Replace the values in the 'label' column

for lang in languages:

  predict_folder = f"data/{lang}/test-articles-subtask-1/"

  df_pred_lang = make_dataframe(predict_folder, labels_folder = None)

  data_pred_dict[lang] = {'pred': df_pred_lang}

# All labels

pred_datasets = {}

for key,el in data_pred_dict.items():
  pred_dataset = Dataset.from_pandas(el['pred'], preserve_index=True)

  pred_datasets[key] = pred_dataset

pred_tokenized_datasets = {}

for key,el in pred_datasets.items():
    pred_tokenized_datasets[key] = el.map(tokenize_function, batched=True, remove_columns=["text"])

# pred_results = {}

for language, model_trainer in zip(languages, models):
    pred_ans = model_trainer.predict(pred_tokenized_datasets[language])

    max_indices = np.argmax(pred_ans[0], axis=1)
    indexes = data_pred_dict[language]['pred'].index.tolist()

    pred_ans_df = pd.DataFrame({'Index': indexes, 'Value': max_indices})

    pred_ans_df['Value'] = pred_ans_df['Value'].replace(label_mapping)


    # pred_results[language] = pred_ans_df

    pred_ans_df.to_csv(f'{language}_test_sumbission.txt', sep='\t', index=False, header=False)

