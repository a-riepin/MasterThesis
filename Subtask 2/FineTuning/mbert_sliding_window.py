# -*- coding: utf-8 -*-
"""mBert_sliding_window.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mUb1Y9ntxyPF4DlbOWgpUecnnU8DH5mm
"""

!pip install scikit-multilearn
!pip install -q transformers datasets evaluate accelerate

from google.colab import drive
drive.mount('/content/drive')

!unzip -qq '/content/drive/MyDrive/data.zip'

import os
import re
import string
import json
import numpy as np
import pandas as pd
from sklearn import metrics
from bs4 import BeautifulSoup
import transformers
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW
import warnings
from pathlib import Path
from tqdm import tqdm
from sklearn.model_selection import train_test_split, ParameterGrid
from typing import Optional, List, Union, Tuple, Dict

warnings.filterwarnings('ignore')

from pathlib import Path
import pandas as pd
from skmultilearn.model_selection import iterative_train_test_split
from sklearn.model_selection import train_test_split
from typing import Optional
import os
from tqdm import tqdm

base_data_path = Path("data")
data_folder = {
    "train": "train-articles-subtask-2",
    "dev": "dev-articles-subtask-2",
    "test": "test-articles-subtask-2"
}

labels_file_paths = {
    "train": "train-labels-subtask-2.txt",
    "dev": "dev-labels-subtask-2.txt",
    "test": "test-labels-subtask-2.txt"
}

collected_data = {
    "train": None, "dev": None, "test": None
}

def merge_database(old: Optional[pd.DataFrame], new: pd.DataFrame) -> pd.DataFrame:
    if old is None:
        return new
    return pd.concat(objs=[old, new], axis="index", ignore_index=False, verify_integrity=True, sort=False)

def make_dataframe(input_folder, labels_folder=None):
    #MAKE TXT DATAFRAME
    text = []
    for fil in tqdm(filter(lambda x: x.endswith('.txt'), os.listdir(input_folder))):
        iD, txt = fil[7:].split('.')[0], open(str(input_folder) + '/' + fil, 'r', encoding='utf-8').read()
        text.append((iD, txt))
    df_text = pd.DataFrame(text, columns=['id','text']).set_index('id')
    df = df_text

    #MAKE LABEL DATAFRAME
    if labels_folder:
        labels = pd.read_csv(labels_folder, sep='\t', header=None)
        labels = labels.rename(columns={0:'id',1:'frames'})
        labels.id = labels.id.apply(str)
        labels = labels.set_index('id')
        #JOIN
        df = labels.join(df_text)[['text','frames']]
    return df

LANGUAGE_ABBREVIATION_TO_FULL = {
    "en": "english",
    "fr": "french",
    "it": "italian",
    "ru": "russian",
    "po": "polish",
    "ge": "german",
    "es": "spanish",
    "gr": "greek",
    "ka": "georgian",
    "es2en": "english",
    "fr2en": "english",
    "ge2en": "english",
    "gr2en": "english",
    "it2en": "english",
    "ka2en": "english",
    "po2en": "english",
    "ru2en": "english"
}

LANGUAGE_FULL_TO_ABBREVIATION = {v: k for k, v in LANGUAGE_ABBREVIATION_TO_FULL.items()}

def get_pure_language_abbreviation(language_tag: str) -> str:
    if "2" in language_tag:
        return language_tag.split(sep="2")[-1]
    return language_tag

languages = ['en', 'fr', 'ge', 'it', 'po', 'ru']

for language in languages:
    if len(language) > 2 and not language.endswith("2en"):
        try:
            language = LANGUAGE_FULL_TO_ABBREVIATION[language]
        except KeyError:
            continue

    for split in ("train", "dev"):
        input_folder = base_data_path.joinpath(language, data_folder[split])
        labels_file = base_data_path.joinpath(language, labels_file_paths[split])
        if not input_folder.exists():
            continue

        df = make_dataframe(
            input_folder=input_folder,
            labels_folder=str(labels_file.absolute()) if labels_file.exists() else None
        )
        df["language"] = language

        try:
            collected_data[split] = merge_database(old=collected_data[split], new=df)
        except ValueError:
            collected_data[split] = df

df_train = collected_data["train"]
df_dev = collected_data["dev"]

df_train = pd.concat([df_train, df_dev], ignore_index=True)
df_train['frames_list'] = df_train['frames'].apply(lambda x: [frame.strip() for frame in x.split(',')])

frames_list = [
    "Economic",
    "Capacity_and_resources",
    "Morality",
    "Fairness_and_equality",
    "Legality_Constitutionality_and_jurisprudence",
    "Policy_prescription_and_evaluation",
    "Crime_and_punishment",
    "Security_and_defense",
    "Health_and_safety",
    "Quality_of_life",
    "Cultural_identity",
    "Public_opinion",
    "Political",
    "External_regulation_and_reputation"
]

# Explode the frames_list column to have one frame per row
df_exploded = df_train.explode('frames_list')

# Group by language and frames_list, then count occurrences
df_counts = df_exploded.groupby(['language', 'frames_list']).size().reset_index(name='count')

# Display the resulting DataFrame
display(df_counts)

# Count the number of rows in df_counts per language
df_language_counts = df_counts.groupby('language').size().reset_index(name='row_count')

# Display the resulting DataFrame
display(df_language_counts)

def idx2class(idx_list):
    return [frames_list.index(i) for i in idx_list]

df_train['Emotions'] = df_train['frames_list'].apply(idx2class)

for frame in frames_list:
    df_train[frame] = df_train['frames_list'].apply(lambda x: 1 if frame in x else 0)

df_train.drop(['frames', 'frames_list', 'language', 'Emotions'], axis=1, inplace=True)

device = 'cuda' if torch.cuda.is_available() else 'cpu'

df_train

target_cols = [col for col in df_train.columns if col not in ['text', 'id']]
df_train['labels'] = df_train[target_cols].values.tolist()

# Ensure the labels are in a multi-label format
X = df_train['text'].values.reshape(-1, 1)  # Features
y = df_train[target_cols].values  # Labels

# Split the data using iterative_train_test_split
X_train, y_train, X_dev, y_dev = iterative_train_test_split(X, y, test_size=0.2)

# Convert the splits back to DataFrames
df_train = pd.DataFrame({'text': X_train.flatten(), 'labels': list(y_train)})
df_dev = pd.DataFrame({'text': X_dev.flatten(), 'labels': list(y_dev)})

# Proceed with further processing
print(df_train.head())
print(df_dev.head())

# Convert the splits back to DataFrames
df_train = pd.DataFrame({'text': X_train.flatten(), 'labels': list(y_train)})
df_dev = pd.DataFrame({'text': X_dev.flatten(), 'labels': list(y_dev)})

# Add back the columns for each frame
for i, frame in enumerate(frames_list):
    df_train[frame] = df_train['labels'].apply(lambda x, i=i: x[i])
    df_dev[frame] = df_dev['labels'].apply(lambda x, i=i: x[i])

# Proceed with further processing
display(df_train.head())
display(df_dev.head())

tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')

class BERTDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.df = df
        self.max_len = max_len
        self.text = df.text
        self.tokenizer = tokenizer
        self.targets = df[target_cols].values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        text = self.text[index]
        inputs = self.tokenizer.encode_plus(
            text,
            truncation=True,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }

class BERTClass(nn.Module):
    def __init__(self, window_size, stride):
        super(BERTClass, self).__init__()
        self.bert = AutoModel.from_pretrained('bert-base-multilingual-uncased')
        self.fc = nn.Linear(768, 14)
        self.window_size = window_size
        self.stride = stride

    def forward(self, ids, mask, token_type_ids):
        outputs = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=True)
        embeddings = outputs.last_hidden_state

        window_size = self.window_size
        stride = self.stride

        batch_size, max_len, embedding_dim = embeddings.size()
        num_windows = ((max_len - window_size) // stride) + 1

        window_outputs = []
        for i in range(num_windows):
            start_idx = i * stride
            end_idx = start_idx + window_size
            window = embeddings[:, start_idx:end_idx, :]

            window_avg_pool = window.mean(dim=1)
            window_outputs.append(window_avg_pool)

        window_outputs = torch.cat(window_outputs, dim=1)
        output = self.fc(window_outputs)

        return output

def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)

def train(epoch, model, train_loader):
    model.train()
    for _, data in enumerate(train_loader, 0):
        ids = data['ids'].to(device, dtype=torch.long)
        mask = data['mask'].to(device, dtype=torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)
        targets = data['targets'].to(device, dtype=torch.float)

        outputs = model(ids, mask, token_type_ids)
        loss = loss_fn(outputs, targets)
        if _ % 500 == 0:
            print(f'Epoch: {epoch}, Loss:  {loss.item()}')

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

def validation(model, valid_loader):
    model.eval()
    fin_targets = []
    fin_outputs = []
    with torch.no_grad():
        for _, data in enumerate(valid_loader, 0):
            ids = data['ids'].to(device, dtype=torch.long)
            mask = data['mask'].to(device, dtype=torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)
            targets = data['targets'].to(device, dtype=torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs, fin_targets

# Define the hyperparameter grid for tuning
hyperparameter_grid = {
    "num_train_epochs": [10, 15, 30],
    "learning_rate": [ 2e-5, 3e-5, 4e-5],
    "per_device_batch_size": [8, 16, 32],
    "max_len": [200, 300, 500],
    "stride": [50, 100],
}

best_params_global = None
best_f1_macro_global = 0.0
all_results = []

best_evaluate_results = {}

# Lists to store results for plotting
iterations = []
avg_f1_macros = []

models = []

# Perform grid search
for i, params in enumerate(ParameterGrid(hyperparameter_grid)):
    print('-----' * 4)
    print(f"Iteration {i + 1}: Training with hyperparameters {params}")

    EPOCHS = params['num_train_epochs']
    LEARNING_RATE = params['learning_rate']
    TRAIN_BATCH_SIZE = params['per_device_batch_size']
    VALID_BATCH_SIZE = params['per_device_batch_size']
    MAX_LEN = params['max_len']
    STRIDE = params['stride']

    train_dataset = BERTDataset(df_train, tokenizer, MAX_LEN)
    valid_dataset = BERTDataset(df_dev, tokenizer, MAX_LEN)
    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE,
                              num_workers=4, shuffle=True, pin_memory=True)
    valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE,
                              num_workers=4, shuffle=False, pin_memory=True)

    model = BERTClass(MAX_LEN, STRIDE)
    model.to(device)

    optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)

    for epoch in range(EPOCHS):
        train(epoch, model, train_loader)

    outputs, targets = validation(model, valid_loader)
    outputs = np.array(outputs) >= 0.5
    accuracy = metrics.accuracy_score(targets, outputs)
    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')
    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')
    print(f"Accuracy Score = {accuracy}")
    print(f"F1 Score (Micro) = {f1_score_micro}")
    print(f"F1 Score (Macro) = {f1_score_macro}")

    all_results.append({
        'hyperparameters': params,
        'f1_macro': f1_score_macro,
        'f1_micro': f1_score_micro
    })
    iterations.append(str(params))

    models.append(model)

params_plot = [str(d['hyperparameters']) for d in all_results]
f1_macro_plot = [d['f1_macro'] for d in all_results]
f1_micro_plot = [d['f1_micro'] for d in all_results]

import matplotlib.pyplot as plt

# Visualize results
plt.plot(params_plot, f1_macro_plot, marker='o', color='blue')
plt.xticks(rotation=90)  # Rotate x-axis tick labels

# Visualize results
plt.plot(params_plot, f1_micro_plot, marker='o', color='red')
plt.xlabel('Iteration')
plt.ylabel(' F1 ')
plt.title('Grid Search for Hyperparameter Tuning')
plt.xticks(rotation=90)  # Rotate x-axis tick labels


plt.show()

f1_micro_plot

params_plot

best_model = models[3]

"""# Predict

"""

test_data = pd.DataFrame()

class BERTTest(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.df = df
        self.max_len = max_len
        self.text = df.text
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        text = self.text[index]
        inputs = self.tokenizer.encode_plus(
            text,
            truncation=True,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
        }

def predict(model, test_loader):
    model.eval()
    test_outputs=[]
    with torch.no_grad():
        for _, data in enumerate(test_loader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            outputs = best_model(ids, mask, token_type_ids)

            test_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return test_outputs

languages = [ 'en','es', 'fr', 'ge', 'gr', 'it', 'ka', 'po', 'ru']

for language in languages:
    if len(language) > 2 and not language.endswith("2en"):
        try:
            language = LANGUAGE_FULL_TO_ABBREVIATION[language]
        except KeyError:
            continue

    split = "test"
    input_folder = base_data_path.joinpath(language, data_folder[split])
    labels_file = base_data_path.joinpath(language, labels_file_paths[split])
    if not input_folder.exists():
        continue

    df = make_dataframe(
        input_folder=input_folder,
        labels_folder=str(labels_file.absolute()) if labels_file.exists() else None
    )

    df["language"] = language
    #print(df)

    if not labels_file.exists():
        split = "test"

    try:
        test_data = merge_database(old=test_data, new=df)
    except ValueError:

        test_data = df

test_data.head()

MAX_LEN = 500
TEST_BATCH_SIZE = 30

from tqdm import tqdm

answers = {}

for lang in tqdm(languages):
    df_test = test_data[test_data['language']==lang]

    test_dataset = BERTTest(df_test, tokenizer, MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE,
                              num_workers=4, shuffle=False, pin_memory=True)

    predictions = predict(best_model, test_loader)

    predictions = np.array(predictions) >= 0.5

    answer = [df_test.index, predictions]

    answers[lang] = answer

    boolean_array = answer[1]
    index_values = answer[0]

    answers_test = []
    for arr_bool in boolean_array:

        data=np.array(frames_list)[arr_bool].tolist()
        answers_test.append(data)


    new_list_ans = []

    for ans_test in answers_test:

        my_string = ','.join(ans_test)
        new_list_ans.append(my_string)

    # Convert to DataFrame
    df = pd.DataFrame(data=new_list_ans, index=index_values)
    df.to_csv(f'{lang}_bert_sliding_window.txt', sep='\t', header=False)